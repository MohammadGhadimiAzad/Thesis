!pip install yfinance tensorflow --quiet

# integrated_cpoa_with_indicators_and_gru.py
# Ø§Ø¬Ø±Ø§ Ø¯Ø± Colab: !pip install yfinance tensorflow --quiet
# Ø³Ù¾Ø³: python integrated_cpoa_with_indicators_and_gru.py
import os
import warnings
warnings.filterwarnings("ignore")

# --- Ù†ØµØ¨ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ (Ø¯Ø± Ù…Ø­ÛŒØ· ØªØ¹Ø§Ù…Ù„ÛŒ Ù…Ø«Ù„ Colab uncomment Ú©Ù†ÛŒØ¯) ---
# !pip install yfinance tensorflow --quiet

import numpy as np
import pandas as pd
import yfinance as yf
from datetime import datetime, timedelta
import matplotlib.pyplot as plt

# Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Deep Learning
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import GRU, Dense
    from sklearn.preprocessing import MinMaxScaler
    TF_AVAILABLE = True
except Exception:
    TF_AVAILABLE = False

np.random.seed(42)

# ---------------------------
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú©Ø§Ø±Ø¨Ø±ÛŒ
# ---------------------------
SYMBOLS = ["BTC-USD", "ETH-USD", "LTC-USD"]  # Ù…Ø«Ø§Ù„: Ú†Ù†Ø¯ Ø¯Ø§Ø±Ø§ÛŒÛŒ Ø§Ù†ØªØ®Ø§Ø¨ÛŒ
START = "2022-01-01"
END = datetime.now().strftime("%Y-%m-%d")
LOOKBACK_DAYS = 30
MIN_DAYS_FOR_FEATURES = 120

USE_DL = True and TF_AVAILABLE     # Ø§Ú¯Ø± TF Ù†ØµØ¨ Ù†Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø®Ø§Ù…ÙˆØ´ Ù…ÛŒâ€ŒØ´ÙˆØ¯
EPOCHS = 10        # Ø¨Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´ Ú©ÙˆØªØ§Ù‡; Ø¯Ø± Ú©Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯
TIME_STEP = 14     # Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ GRU (Ù…Ø«Ù„Ø§Ù‹ 14 Ø±ÙˆØ²)
OUTPUT_CSV = "pareto_front_results.csv"

# ---------------------------
# ØªÙˆØ§Ø¨Ø¹ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§
# ---------------------------
def calculate_bollinger_bands(df, window=20, num_std=2):
    df = df.copy()
    df['SMA'] = df['Close'].rolling(window=window).mean()
    df['STD'] = df['Close'].rolling(window=window).std()
    df['BB_Upper'] = df['SMA'] + (df['STD'] * num_std)
    df['BB_Lower'] = df['SMA'] - (df['STD'] * num_std)
    return df

def calculate_rsi(df, window=14):
    df = df.copy()
    delta = df['Close'].diff()
    gain = delta.where(delta > 0, 0).rolling(window=window).mean()
    loss = -delta.where(delta < 0, 0).rolling(window=window).mean()
    rs = gain / (loss + 1e-9)
    df['RSI'] = 100 - (100 / (1 + rs))
    return df

def calculate_ichimoku(df):
    df = df.copy()
    high = df['High']; low = df['Low']; close = df['Close']
    df['Tenkan_sen'] = (high.rolling(9).max() + low.rolling(9).min()) / 2
    df['Kijun_sen'] = (high.rolling(26).max() + low.rolling(26).min()) / 2
    df['Senkou_A'] = ((df['Tenkan_sen'] + df['Kijun_sen']) / 2).shift(26)
    df['Senkou_B'] = ((high.rolling(52).max() + low.rolling(52).min()) / 2).shift(26)
    df['Chikou'] = close.shift(-26)
    return df

def calculate_ema(df, span=20):
    df = df.copy()
    df[f'EMA_{span}'] = df['Close'].ewm(span=span, adjust=False).mean()
    return df

def calculate_basic_features(df):
    df = df.copy()
    df['momentum_7d'] = df['Close'].pct_change(7)
    df['volatility_30d'] = df['Close'].pct_change().rolling(30).std()
    df['return_1d'] = df['Close'].pct_change(1)
    return df

# ---------------------------
# Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ GRU (ÛŒÚ©Ø¨Ø§Ø± Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø³Ù¾Ø³ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø®Ø´ ØªØ³Øª)
# ---------------------------
def create_gru_model(input_shape):
    model = Sequential()
    model.add(GRU(64, return_sequences=True, input_shape=input_shape))
    model.add(GRU(32, return_sequences=False))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

def train_predict_gru(close_series, time_step=TIME_STEP, epochs=EPOCHS):
    """
    ÙˆØ±ÙˆØ¯ÛŒ: close_series: pd.Series indexed by date
    Ø®Ø±ÙˆØ¬ÛŒ: pd.Series Ø§Ø² 'predicted_return' Ø¨Ø§ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø®Ø´ÛŒ Ø§Ø² ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§ (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ø®Ø´ ØªØ³Øª)
    Ø±ÙˆØ´: Ø¯Ø§Ø¯Ù‡ Ø±Ùˆ 80/20 ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŒ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¨Ø®Ø´ Ø¢Ù…ÙˆØ²Ø´ Ø¢Ù…ÙˆØ²Ø´ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯ Ùˆ Ø±ÙˆÛŒ Ø¨Ø®Ø´ ØªØ³Øª Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    arr = close_series.values.reshape(-1, 1)
    scaler = MinMaxScaler()
    arr_scaled = scaler.fit_transform(arr)
    
    train_size = int(len(arr_scaled) * 0.8)
    if train_size <= time_step + 1:
        # Ø¯Ø§Ø¯Ù‡ Ú©Ù… Ø§Ø³ØªØ› Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ú©Ù†
        return pd.Series(dtype=float, index=close_series.index)
    
    X_train, y_train = [], []
    for i in range(time_step, train_size):
        X_train.append(arr_scaled[i-time_step:i, 0])
        y_train.append(arr_scaled[i, 0])
    X_train = np.array(X_train).reshape((-1, time_step, 1))
    y_train = np.array(y_train)
    
    X_test, test_idx = [], []
    for i in range(train_size, len(arr_scaled)):
        if i - time_step < 0:
            continue
        X_test.append(arr_scaled[i-time_step:i, 0])
        test_idx.append(close_series.index[i])
    if len(X_test) == 0:
        return pd.Series(dtype=float, index=close_series.index)
    X_test = np.array(X_test).reshape((-1, time_step, 1))
    
    if USE_DL and TF_AVAILABLE:
        model = create_gru_model((time_step, 1))
        model.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=0)
        preds_scaled = model.predict(X_test).reshape(-1, 1)
        preds = scaler.inverse_transform(preds_scaled).reshape(-1)
    else:
        # fallback Ø³Ø§Ø¯Ù‡: Ø§Ø² Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØºÛŒÛŒØ±Ø§Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù† (baseline)
        last_values = [arr.flatten()[i-time_step:i] for i in range(train_size, len(arr_scaled))]
        preds = [v[-1] for v in last_values]
    
    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¨Ø§Ø²Ø¯Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡: (pred - prev)/prev
    prevs = []
    for i in range(train_size, len(arr_scaled)):
        prevs.append(close_series.values[i-1])
    preds_returns = (np.array(preds).astype(float) - np.array(prevs).astype(float)) / (np.array(prevs).astype(float) + 1e-9)
    
    result_series = pd.Series(preds_returns, index=test_idx)
    result_series.name = "predicted_return"
    return result_series

# ---------------------------
# Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙÛŒÚ†Ø±Ù‡Ø§
# ---------------------------
def download_and_compute(symbols, start=START, end=END):
    historical = {}
    for s in symbols:
        try:
            df = yf.download(s, start=start, end=end, progress=False, auto_adjust=False)
            if df.shape[0] < MIN_DAYS_FOR_FEATURES:
                print(f"  âš ï¸ {s}: Ø¯Ø§Ø¯Ù‡ Ù†Ø§Ú©Ø§ÙÛŒ ({len(df)} Ø±ÙˆØ²) â€” Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯")
                continue
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§
            df = calculate_basic_features(df)
            df = calculate_bollinger_bands(df)
            df = calculate_rsi(df)
            df = calculate_ichimoku(df)
            df = calculate_ema(df, span=20)
            historical[s] = df
            print(f"  âœ… {s}: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ ÙÛŒÚ†Ø±Ù‡Ø§ Ø¢Ù…Ø§Ø¯Ù‡ ({len(df)} Ø±ÙˆØ²)")
        except Exception as e:
            print(f"  âŒ {s}: Ø®Ø·Ø§ - {e}")
    return historical

# ---------------------------
# ØªØ¨Ø¯ÛŒÙ„ ÙÛŒÚ†Ø±Ù‡Ø§ Ø¨Ù‡ ÙØ±Ù…Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² CPOA Ùˆ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§
# ---------------------------
def add_predicted_feature_to_hist(historical):
    predictions = {}
    for s, df in historical.items():
        predicted = train_predict_gru(df['Close'], time_step=TIME_STEP, epochs=EPOCHS)
        # Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¯Ø± Ù‡Ù…Ø§Ù† Ø§ÛŒÙ†Ø¯Ú©Ø³ (Ø±ÛŒØ¨Ø§ Ø´Ø¯Ù‡)
        historical[s] = historical[s].copy()
        historical[s]['predicted_return'] = np.nan
        # only fill where predictions exist
        for idx, val in predicted.items():
            if idx in historical[s].index:
                historical[s].loc[idx, 'predicted_return'] = val
        predictions[s] = predicted
        print(f"  ðŸ”® {s}: predicted feature length = {len(predicted)}")
    return historical, predictions

# ---------------------------
# Ú©Ù„Ø§Ø³ MultiObjectiveCPOA (ØªØ¹Ø¯ÛŒÙ„ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ±)
# ---------------------------
class MultiObjectiveCPOA:
    def __init__(self, population_size=20, max_generations=6, transaction_cost=0.001, lookback_days=LOOKBACK_DAYS):
        self.population_size = population_size
        self.max_generations = max_generations
        self.transaction_cost = transaction_cost
        self.lookback_days = lookback_days
        self.symbols = None
        self.num_assets = 0

    def initialize_population(self):
        pop = []
        for _ in range(self.population_size):
            w = np.random.dirichlet(np.ones(self.num_assets))
            strat = {
                'weights': w,
                'volatility_threshold': float(np.random.uniform(0.02, 0.1)),
                'risk_appetite': float(np.random.uniform(0.2, 1.2)),
                'rebalance_frequency': int(np.random.choice([7, 14, 30]))
            }
            pop.append(strat)
        return pop

    def evaluate_strategy(self, strategy, features_list, price_data):
        dates = price_data.index.to_list()
        if len(dates) < self.lookback_days + 2:
            return -1000.0, -1000.0

        rets = []
        prev_weights = np.zeros(self.num_assets)

        for t in range(self.lookback_days, len(dates)):
            cur_date = dates[t]
            prev_date = dates[t - 1]
            daily_ret = 0.0

            for i, sym in enumerate(self.symbols):
                feats = features_list[i]
                if cur_date not in feats.index:
                    continue
                # Ø®ÙˆØ§Ù†Ø¯Ù† ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
                try:
                    mom = feats.loc[cur_date, 'momentum_7d']
                    vol = feats.loc[cur_date, 'volatility_30d']
                    rsi = feats.loc[cur_date, 'RSI']
                    pred = feats.loc[cur_date, 'predicted_return'] if 'predicted_return' in feats.columns else np.nan
                    bb_u = feats.loc[cur_date, 'BB_Upper']
                    bb_l = feats.loc[cur_date, 'BB_Lower']
                    ema20 = feats.loc[cur_date, 'EMA_20']
                except Exception:
                    continue

                if pd.isna(mom) or pd.isna(vol):
                    continue
                w = strategy['weights'][i]

                # ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ: Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø³Ø§Ø¯Ù‡ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± ÙÛŒÚ†Ø±Ù‡Ø§
                score = 0.0
                # RSI
                if not pd.isna(rsi):
                    if rsi < 30:
                        score += 0.6
                    elif rsi > 70:
                        score -= 0.6
                # Bollinger
                price_now = price_data.loc[cur_date, sym]
                price_prev = price_data.loc[prev_date, sym]
                if not pd.isna(price_now) and not pd.isna(price_prev) and price_prev != 0:
                    # oversold if price < lower band
                    if not pd.isna(bb_l) and price_now < bb_l:
                        score += 0.5
                    if not pd.isna(bb_u) and price_now > bb_u:
                        score -= 0.5
                # predicted_return
                if not pd.isna(pred):
                    score += float(pred) * 2.0  # ÙˆØ²Ù† Ø¯Ø§Ø¯Ù† Ø¨Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…Ø¯Ù„

                # volatility-based penalization
                if not pd.isna(vol) and vol > strategy['volatility_threshold']:
                    score -= 0.3

                asset_ret = 0.0
                try:
                    asset_ret = (price_now / price_prev) - 1.0
                except Exception:
                    asset_ret = 0.0

                # final contribution: ÙˆØ²Ù† * risk_appetite * (asset_ret + score factor)
                daily_ret += w * strategy['risk_appetite'] * (asset_ret + 0.5 * score)

            # Ù‡Ø²ÛŒÙ†Ù‡ ØªØ±Ø§Ú©Ù†Ø´ Ø¯Ø± rebalance
            if (t - self.lookback_days) % int(strategy['rebalance_frequency']) == 0:
                turnover = np.sum(np.abs(strategy['weights'] - prev_weights))
                cost = self.transaction_cost * turnover
                daily_ret -= cost
                prev_weights = strategy['weights'].copy()

            rets.append(daily_ret)

        if len(rets) == 0:
            return -1000.0, -1000.0

        rets = pd.Series(rets)
        eps = 1e-9
        sharpe = (rets.mean() / (rets.std() + eps)) * np.sqrt(252)  # annualized
        total = (1 + rets).prod() - 1
        return float(sharpe), float(total)

    def dominates(self, a, b):
        return (a[0] >= b[0] and a[1] >= b[1]) and (a[0] > b[0] or a[1] > b[1])

    def pareto_front(self, scores):
        front = []
        for i, s in enumerate(scores):
            dominated = False
            for j, t in enumerate(scores):
                if i != j and self.dominates(t, s):
                    dominated = True
                    break
            if not dominated:
                front.append(i)
        return front

    def mutate(self, strategy):
        new_w = strategy['weights'] + np.random.normal(0, 0.05, size=self.num_assets)
        new_w = np.clip(new_w, 0, None)
        if new_w.sum() <= 0:
            new_w = np.ones_like(new_w) / len(new_w)
        else:
            new_w = new_w / new_w.sum()
        new_s = strategy.copy()
        new_s['weights'] = new_w
        # ØªØºÛŒÛŒØ± Ú©ÙˆÚ†Ú© Ø¯Ø± Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
        new_s['volatility_threshold'] = float(np.clip(new_s['volatility_threshold'] + np.random.normal(0, 0.005), 0.01, 0.5))
        new_s['risk_appetite'] = float(np.clip(new_s['risk_appetite'] + np.random.normal(0, 0.05), 0.05, 2.0))
        return new_s

    def optimize(self, historical_data, min_days=MIN_DAYS_FOR_FEATURES):
        print("ðŸ” Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙÛŒÚ†Ø±Ù‡Ø§ Ùˆ Ù‡Ù…â€ŒØªØ±Ø§Ø²Ø³Ø§Ø²ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§...")
        features_dict = {}
        price_series_dict = {}
        valid_symbols = []

        for sym, df in historical_data.items():
            if 'Close' not in df.columns:
                continue
            f = df[['momentum_7d','volatility_30d','RSI','BB_Upper','BB_Lower','EMA_20','predicted_return','return_1d']].copy()
            f_clean = f.dropna(how='any')
            if len(f_clean) < min_days:
                print(f"  âš ï¸ {sym}: Ø¯Ø§Ø¯Ù‡ ÙÛŒÚ†Ø± Ù†Ø§Ú©Ø§ÙÛŒ ({len(f_clean)} Ø±ÙˆØ²) â€” Ú†Ø´Ù…â€ŒÙ¾ÙˆØ´ÛŒ Ø´Ø¯")
                continue
            features_dict[sym] = f_clean
            price_series_dict[sym] = df['Close'].reindex(f_clean.index)
            valid_symbols.append(sym)

        if len(valid_symbols) < 2:
            raise RuntimeError("Ø¯Ø§Ø¯Ù‡Ù” Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø­Ø¯Ø§Ù‚Ù„ 2 Ø¯Ø§Ø±Ø§ÛŒÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª. Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù…Ø´ØªØ±Ú©
        common_index = None
        for sym in valid_symbols:
            idx = features_dict[sym].index
            if common_index is None:
                common_index = idx
            else:
                common_index = common_index.intersection(idx)
        if common_index is None or len(common_index) < self.lookback_days + 2:
            raise RuntimeError("Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù…Ø´ØªØ±Ú© Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ Ø§Ø³ØªØ› Ø¨Ø§Ø²Ù‡Ù” Ø²Ù…Ø§Ù†ÛŒ ÛŒØ§ Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø±Ø§ ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯.")

        # Ø³Ø§Ø®Øª price_data
        price_data_dict = {}
        for sym in valid_symbols:
            price_series = price_series_dict[sym].reindex(common_index)
            price_data_dict[sym] = price_series
        price_data = pd.DataFrame(price_data_dict).dropna(how='any')
        if len(price_data) < self.lookback_days + 2:
            raise RuntimeError("Ù¾Ø³ Ø§Ø² Ù‡Ù…â€ŒØªØ±Ø§Ø²ÛŒØŒ Ø·ÙˆÙ„ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª.")

        # Ù‡Ù…â€ŒØªØ±Ø§Ø²Ø³Ø§Ø²ÛŒ features_list
        features_list = []
        for sym in valid_symbols:
            feats = features_dict[sym].reindex(price_data.index)
            features_list.append(feats)

        self.symbols = valid_symbols
        self.num_assets = len(features_list)
        print(f"âœ… Ø¯Ø§Ø±Ø§ÛŒÛŒâ€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡: {self.num_assets}, Ø·ÙˆÙ„ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ù†Ù‡Ø§ÛŒÛŒ: {len(price_data)}")

        # Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ØªÚ©Ø§Ù…Ù„ÛŒ
        population = self.initialize_population()
        for gen in range(self.max_generations):
            scores = []
            for s in population:
                try:
                    score = self.evaluate_strategy(s, features_list, price_data)
                    scores.append(score)
                except Exception as e:
                    print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ: {e}")
                    scores.append((-1000.0, -1000.0))

            front_idx = self.pareto_front(scores)
            front_strats = [population[i] for i in front_idx]
            new_pop = front_strats.copy()
            # ØªÙˆÙ„ÛŒØ¯ ÙØ±Ø²Ù†Ø¯Ø§Ù† ØªØ§ Ú©Ø§Ù…Ù„ Ø´Ø¯Ù† Ø¬Ù…Ø¹ÛŒØª
            while len(new_pop) < self.population_size:
                parent = population[np.random.randint(0, len(population))]
                new_pop.append(self.mutate(parent))
            population = new_pop
            print(f"Ù†Ø³Ù„ {gen+1}/{self.max_generations}: Ø¬Ù…Ø¹ÛŒØª={len(population)}, Ø¹Ø¯Ø¯ Ø¯Ø± Ù…Ø±Ø² Ù¾Ø§Ø±ØªÙˆ={len(front_idx)}")

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ù…Ø±Ø² Ù¾Ø§Ø±ØªÙˆ
        final_scores = []
        for s in population:
            try:
                score = self.evaluate_strategy(s, features_list, price_data)
                final_scores.append(score)
            except Exception:
                final_scores.append((-1000.0, -1000.0))
        final_front_idx = self.pareto_front(final_scores)
        pareto_set = [(population[i], final_scores[i]) for i in final_front_idx]

        return pareto_set, valid_symbols

# ---------------------------
# Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ
# ---------------------------
if __name__ == "__main__":
    print("ðŸ“¥ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙÛŒÚ†Ø±Ù‡Ø§...")
    hist = download_and_compute(SYMBOLS, start=START, end=END)
    if len(hist) < 2:
        raise SystemExit("Ø¯ÛŒØªØ§ÛŒ Ú©Ø§ÙÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ùˆ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")
    print("\nðŸ”® Ø³Ø§Ø®Øª ÙÛŒÚ†Ø± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ (Ù…Ø¯Ù„ GRU / fallback) ...")
    hist, preds = add_predicted_feature_to_hist(hist)

    # Ø§Ø¬Ø±Ø§ÛŒ CPOA
    print("\nâš™ï¸ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú†Ù†Ø¯Ù‡Ø¯ÙÙ‡ (CPOA)...")
    cpoa = MultiObjectiveCPOA(population_size=20, max_generations=5, transaction_cost=0.001, lookback_days=LOOKBACK_DAYS)
    pareto_set, used_symbols = cpoa.optimize(hist, min_days=MIN_DAYS_FOR_FEATURES)

    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    rows = []
    for strat, (sharpe, tot) in pareto_set:
        rows.append({
            "Sharpe": sharpe,
            "TotalReturn": tot,
            "VolatilityThreshold": strat['volatility_threshold'],
            "RiskAppetite": strat['risk_appetite'],
            "RebalanceFreq": strat['rebalance_frequency'],
            "Weights": ",".join([f"{w:.6f}" for w in strat['weights']])
        })
    df_out = pd.DataFrame(rows)
    if not df_out.empty:
        df_out.to_csv(OUTPUT_CSV, index=False)
        print(f"\nâœ… Ù†ØªØ§ÛŒØ¬ Ù…Ø±Ø² Ù¾Ø§Ø±ØªÙˆ Ø¯Ø±: {OUTPUT_CSV}")
        # Ù†Ù…ÙˆØ¯Ø§Ø±
        plt.figure(figsize=(8,6))
        plt.scatter(df_out["Sharpe"], df_out["TotalReturn"], alpha=0.7)
        plt.xlabel("Sharpe Ratio"); plt.ylabel("Total Return"); plt.title("Pareto Front (Sharpe vs Total Return)")
        plt.grid(True, alpha=0.3)
        plt.show()
        best_idx = df_out["Sharpe"].idxmax()
        best = df_out.loc[best_idx]
        print(f"\nðŸ† Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ (Ø¨Ø±Ø§Ø³Ø§Ø³ Sharpe):")
        print(best.to_string())
    else:
        print("Ù‡ÛŒÚ† Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø±ÙˆÛŒ Ù…Ø±Ø² Ù¾Ø§Ø±ØªÙˆ ØªÙˆÙ„ÛŒØ¯ Ù†Ø´Ø¯.")
